{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load mCLIP from FLAME\n",
    "\n",
    "## Issues\n",
    "* !pip install rich\n",
    "* https://github.com/Lightning-Universe/lightning-bolts/issues/962#issuecomment-1398247929\n",
    "* !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MCLIP(\n",
       "  (text_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (motion_encoder): MotionEncoder(\n",
       "    (input_projection): Linear(in_features=147, out_features=512, bias=True)\n",
       "    (motion_length_emb): Embedding(471, 512)\n",
       "    (motion_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (last_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (lm_projection): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (loss_motion): CrossEntropyLoss()\n",
       "  (loss_text): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.eval_util import load_mclip_model\n",
    "\n",
    "\n",
    "mclip_ckpt = '/home/ctq566/motion-diffusion-model/flame/flame_mclip_hml3d_bc.ckpt'\n",
    "mclip = load_mclip_model(mclip_ckpt)\n",
    "mclip.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../HumanML3D/')\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from common.skeleton import Skeleton\n",
    "from common.quaternion import *\n",
    "from paramUtil import *\n",
    "import scipy.ndimage.filters as filters\n",
    "\n",
    "\n",
    "n_raw_offsets = torch.from_numpy(t2m_raw_offsets)\n",
    "kinematic_chain = t2m_kinematic_chain\n",
    "# Face direction, r_hip, l_hip, sdr_r, sdr_l\n",
    "face_joint_indx = [2, 1, 17, 16]\n",
    "\n",
    "\n",
    "def get_cont6d_params(positions):\n",
    "    skel = Skeleton(n_raw_offsets, kinematic_chain, \"cpu\")\n",
    "    # (seq_len, joints_num, 4)\n",
    "    quat_params = skel.inverse_kinematics_np(positions, face_joint_indx, smooth_forward=True)\n",
    "\n",
    "    '''Quaternion to continuous 6D'''\n",
    "    cont_6d_params = quaternion_to_cont6d_np(quat_params)\n",
    "    # (seq_len, 4)\n",
    "    r_rot = quat_params[:, 0].copy()\n",
    "    #     print(r_rot[0])\n",
    "    '''Root Linear Velocity'''\n",
    "    # (seq_len - 1, 3)\n",
    "    velocity = (positions[1:, 0] - positions[:-1, 0]).copy()\n",
    "    #     print(r_rot.shape, velocity.shape)\n",
    "    velocity = qrot_np(r_rot[1:], velocity)\n",
    "    '''Root Angular Velocity'''\n",
    "    # (seq_len - 1, 4)\n",
    "    r_velocity = qmul_np(r_rot[1:], qinv_np(r_rot[:-1]))\n",
    "    # (seq_len, joints_num, 4)\n",
    "    return cont_6d_params, r_velocity, velocity, r_rot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import axis_angle_to_matrix, matrix_to_rotation_6d\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def xyz_to_rot6d(xyz):\n",
    "    assert isinstance(xyz, np.ndarray), \"The input should be numpy array\"\n",
    "    assert len(xyz.shape) == 3, \"The shape of xyz should be (seq_len, 22, 3)\"\n",
    "    # Motion length, number of joints, feature dimension\n",
    "    L, J, D = xyz.shape\n",
    "\n",
    "    assert J == 22 and D == 3, \"The shape of xyz should be (seq_len, 22, 3)\"\n",
    "    # Pad to add the missing hand joints and make it 24 joints\n",
    "    # axis_angles = np.pad(xyz, (0, 0, 0, 2, 0, 0), mode='constant', value=0)\n",
    "\n",
    "    # Pad to add the missing hand joints and make it 24 joints using numpy\n",
    "    xyz = np.pad(xyz, ((0, 0), (0, 2), (0, 0)), mode='constant', constant_values=0)\n",
    "    cont_6d_params, r_velocity, velocity, r_rot = get_cont6d_params(xyz)\n",
    "    return cont_6d_params\n",
    "\n",
    "\n",
    "def rot6d_to_flame(rot6d):\n",
    "    assert isinstance(rot6d, np.ndarray), \"The input should be numpy array\"\n",
    "    assert len(rot6d.shape) == 3 and rot6d.shape[1] == 24 and rot6d.shape[2] == 6, \"The shape of rot6d should be (seq_len, 24, 6)\"\n",
    "    # The input is (120, 24, 6), make it (120, 147, 1)\n",
    "    flame = rot6d.reshape(rot6d.shape[0], -1)\n",
    "    flame = np.lib.pad(flame, ((0, 0), (0, 3)), 'constant', constant_values=0)\n",
    "    return flame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 147, 120])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz = np.load('/home/ctq566/motion-diffusion-model/save/p2p_quaternion_2_random/001/sample00_rep00.npy')\n",
    "rot6d = xyz_to_rot6d(xyz)\n",
    "flame = rot6d_to_flame(rot6d)\n",
    "\n",
    "motion = torch.from_numpy(flame).float().to('cuda:0')\n",
    "motion.unsqueeze(0).permute(0, 2, 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerEncoderLayer' object has no attribute 'activation_relu_or_gelu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mclip\u001b[39m.\u001b[39;49mmotion_encoder\u001b[39m.\u001b[39;49mmotion_encoder\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mactivation_relu_or_gelu\n",
      "File \u001b[0;32m~/miniconda3/envs/flame/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[39mfor\u001b[39;00m module_prefix, module \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m     members \u001b[39m=\u001b[39m get_members_fn(module)\n\u001b[0;32m-> 1614\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m members:\n\u001b[1;32m   1615\u001b[0m         \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m v \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m   1616\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerEncoderLayer' object has no attribute 'activation_relu_or_gelu'"
     ]
    }
   ],
   "source": [
    "mclip.motion_encoder.motion_encoder.layers[0].activation_relu_or_gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3644672/375418930.py\", line 2, in <module>\n",
      "    mclip.get_features(motion=motion.unsqueeze(0).permute(0, 2, 1), texts=list(text) * 10, motion_length=torch.tensor([motion.shape[0]], device='cuda:0'))\n",
      "  File \"/home/ctq566/flame/src/models/components/mclip.py\", line 176, in get_features\n",
      "    motion_out = self.motion_encoder(motion, motion_length)\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
      "  File \"/home/ctq566/flame/src/models/components/mclip.py\", line 87, in forward\n",
      "    encoder_out = self.motion_encoder(\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 247, in forward\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\n",
      "    for k, v in members:\n",
      "AttributeError: 'TransformerEncoderLayer' object has no attribute 'activation_relu_or_gelu'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/ctq566/miniconda3/envs/flame/lib/python3.8/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "text = ['A person is walking in the street.']\n",
    "mclip.get_features(motion=motion.unsqueeze(0).permute(0, 2, 1), texts=list(text) * 10, motion_length=torch.tensor([motion.shape[0]], device='cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.my_transformer import TransformerEncoder, TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.components.my_transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "embed_dim = 512\n",
    "n_heads = 8\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "encoder_layers = TransformerEncoderLayer(\n",
    "    d_model=embed_dim,\n",
    "    nhead=n_heads,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layers.activation_relu_or_gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
